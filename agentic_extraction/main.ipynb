{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ce0a86-7905-4811-8011-cbac50ebc08b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Agentic Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13f821bb-2786-4eb0-b239-9116d2521d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cafa51d6-89f6-451f-92a0-5b5ebe932bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install instructor\n",
    "!pip install azure.identity openai\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d1acab-0ab4-4e75-b584-3fa49f02f33d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import List, Dict, Any\n",
    "from core.pipeline import AgenticPipeline\n",
    "from core.bottleneck_registry import BottleneckRegistry\n",
    "from core.models import EvidenceSpan, FinalEvidence\n",
    "from agents.pre_filter import PreFilterAgent\n",
    "from agents.classifier import MultiBottleneckClassifier, ComparativeClassifier\n",
    "from agents.causality_validator import CausalityValidator\n",
    "from agents.arbitrator import EvidenceArbitrator\n",
    "from utils.llm_service import AzureOpenAIService\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07018958-2472-4caa-b595-6b7de27a84e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Initialize Services and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "351efe4e-a91f-420c-82cf-9c68edf75b76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "llm_service = AzureOpenAIService()\n",
    "\n",
    "pre_filter = PreFilterAgent(llm_service)\n",
    "classifier = MultiBottleneckClassifier(llm_service)\n",
    "comparative_classifier = ComparativeClassifier(llm_service)\n",
    "causality_validator = CausalityValidator(llm_service)\n",
    "arbitrator = EvidenceArbitrator(llm_service)\n",
    "\n",
    "registry = BottleneckRegistry()\n",
    "\n",
    "pipeline = AgenticPipeline(\n",
    "    pre_filter=pre_filter,\n",
    "    classifier=classifier,\n",
    "    causality_validator=causality_validator,\n",
    "    arbitrator=arbitrator,\n",
    "    bottleneck_registry=registry,\n",
    "    comparative_classifier=comparative_classifier\n",
    ")\n",
    "\n",
    "print(\"Agentic pipeline initialized successfully\")\n",
    "print(f\"Using model: {config.LLM_MODEL}\")\n",
    "print(f\"Temperature: {config.TEMPERATURE}\")\n",
    "print(f\"Classification threshold: {config.CLASSIFICATION_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "348db49d-43b5-44c1-a82f-a82a00bdd540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d81cb3-2885-472e-9ae9-61dd8488a3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define input parameters\n",
    "COUNTRY_CODES = ['MLI', 'BFA'] \n",
    "MIN_CHUNK_LENGTH = 1000\n",
    "\n",
    "# Load chunks from database\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    c.document_id,\n",
    "    c.chunk_id,\n",
    "    c.chunk_text,\n",
    "    c.page_number,\n",
    "    d.document_name,\n",
    "    d.country_code,\n",
    "    d.document_type,\n",
    "    d.fiscal_year\n",
    "FROM prd_mega.sboost4.per_pfr_chunks c\n",
    "JOIN prd_corpdata.dm_reference_gold.v_dim_imagebank_document d\n",
    "    ON c.document_id = d.document_id\n",
    "WHERE d.country_code IN ({','.join([f\"'{cc}'\" for cc in COUNTRY_CODES])})\n",
    "    AND LENGTH(c.chunk_text) >= {MIN_CHUNK_LENGTH}\n",
    "    AND d.document_type IN ('PER', 'PFR')\n",
    "ORDER BY c.document_id, c.chunk_id\n",
    "LIMIT 5000  -- Limit for testing\n",
    "\"\"\"\n",
    "\n",
    "df_chunks = spark.sql(query).toPandas()\n",
    "print(f\"Loaded {len(df_chunks)} chunks from {df_chunks['document_id'].nunique()} documents\")\n",
    "print(f\"Countries: {df_chunks['country_code'].unique()}\")\n",
    "print(f\"Document types: {df_chunks['document_type'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e997ca-03d0-45f5-91f8-702243e431dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c784bfeb-926f-4944-938e-5f209bf80eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### View Available Bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170edcef-7145-4b52-ba4f-2089e4975ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_bottlenecks = registry.get_all_bottlenecks()\n",
    "\n",
    "print(\"Registered Bottlenecks:\")\n",
    "\n",
    "for b in all_bottlenecks:\n",
    "    print(f\"\\nBottleneck {b['id']}: {b['name']}\")\n",
    "    print(f\"  Category: {b['category']}\")\n",
    "    print(f\"  Description: {b['description'][:150]}...\")\n",
    "    if b.get('examples'):\n",
    "        print(f\"  Example: {b['examples'][0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3bd882-d31b-4537-96d9-9a111735de67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stage 1: Pre-Filtering\n",
    "\n",
    "Fast filtering to identify potentially relevant text spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e79c4ec-9345-4719-a8bf-f113c49bd8e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "chunks_sample = df_chunks.head(BATCH_SIZE).to_dict('records')\n",
    "\n",
    "print(f\"Pre-filtering {len(chunks_sample)} chunks...\")\n",
    "print(\"This stage quickly identifies potentially relevant text spans.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bce0d772-8448-4104-bfad-e7aaee39a286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evidence_spans = pre_filter.filter_batch(chunks_sample)\n",
    "\n",
    "print(f\"\\nPre-filtering Results:\")\n",
    "print(f\"  Input chunks: {len(chunks_sample)}\")\n",
    "print(f\"  Evidence spans found: {len(evidence_spans)}\")\n",
    "print(f\"  Reduction rate: {(1 - len(evidence_spans)/len(chunks_sample))*100:.1f}%\")\n",
    "\n",
    "docs_with_evidence = len(set(span.document_id for span in evidence_spans))\n",
    "print(f\"  Documents with evidence: {docs_with_evidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be2981be-cbc2-4930-a36a-eced97cd31f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if evidence_spans:\n",
    "    print(\"\\nSample Evidence Spans:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, span in enumerate(evidence_spans[:3]):\n",
    "        print(f\"\\n[Span {i+1}]\")\n",
    "        print(f\"  Document: {span.document_name[:50]}...\")\n",
    "        print(f\"  Relevance: {span.relevance_score:.2f}\")\n",
    "        print(f\"  Text: {span.text[:200]}...\")\n",
    "        print(f\"  Potential bottlenecks: {span.potential_bottlenecks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f38283-021b-441d-81d4-252491d25f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stage 2: Multi-Bottleneck Classification\n",
    "\n",
    "Classify each evidence span against ALL bottlenecks simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4f2983b-5f00-412c-966f-b3b31dcf822d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Classify evidence spans\n",
    "classification_results = []\n",
    "\n",
    "print(\"Classifying evidence spans against all bottlenecks...\")\n",
    "print(\"This prevents misclassification by comparing all options.\\n\")\n",
    "\n",
    "for i, span in enumerate(evidence_spans[:10]):  # Process first 10 for demo\n",
    "    print(f\"\\nClassifying span {i+1}/{min(10, len(evidence_spans))}...\")\n",
    "    \n",
    "    # Classify against all bottlenecks\n",
    "    result = classifier.classify(span, all_bottlenecks)\n",
    "    classification_results.append({\n",
    "        'span': span,\n",
    "        'classification': result\n",
    "    })\n",
    "    \n",
    "    if result.top_matches:\n",
    "        print(f\"  Top match: Bottleneck {result.top_matches[0].bottleneck_id} \"\n",
    "              f\"(score: {result.top_matches[0].score:.2f})\")\n",
    "        if len(result.top_matches) > 1:\n",
    "            print(f\"  Second match: Bottleneck {result.top_matches[1].bottleneck_id} \"\n",
    "                  f\"(score: {result.top_matches[1].score:.2f})\")\n",
    "        print(f\"  Ambiguous: {result.is_ambiguous}\")\n",
    "        print(f\"  Recommendation: {result.recommendation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b40b4794-dc32-440e-bfd0-0725a2bee12f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze classification distribution\n",
    "bottleneck_counts = {}\n",
    "ambiguous_count = 0\n",
    "rejected_count = 0\n",
    "\n",
    "for result in classification_results:\n",
    "    if result['classification'].top_matches:\n",
    "        top_match = result['classification'].top_matches[0]\n",
    "        bottleneck_counts[top_match.bottleneck_id] = bottleneck_counts.get(top_match.bottleneck_id, 0) + 1\n",
    "    else:\n",
    "        rejected_count += 1\n",
    "    \n",
    "    if result['classification'].is_ambiguous:\n",
    "        ambiguous_count += 1\n",
    "\n",
    "print(\"\\nClassification Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total classified: {len(classification_results)}\")\n",
    "print(f\"Rejected (no match): {rejected_count}\")\n",
    "print(f\"Ambiguous: {ambiguous_count}\")\n",
    "print(\"\\nBottleneck distribution:\")\n",
    "for bid, count in sorted(bottleneck_counts.items()):\n",
    "    print(f\"  Bottleneck {bid}: {count} matches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52af26ef-2cae-47e1-9b9e-2a4d506411eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stage 3: Handle Ambiguous Classifications\n",
    "\n",
    "Use comparative classifier for ambiguous cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad7c145-c8df-4952-a949-610884e0fdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find ambiguous cases\n",
    "ambiguous_cases = [\n",
    "    r for r in classification_results \n",
    "    if r['classification'].is_ambiguous and len(r['classification'].top_matches) >= 2\n",
    "]\n",
    "\n",
    "if ambiguous_cases:\n",
    "    print(f\"Found {len(ambiguous_cases)} ambiguous cases. Resolving with comparative classifier...\\n\")\n",
    "    \n",
    "    for i, case in enumerate(ambiguous_cases[:3]):  # Process first 3 for demo\n",
    "        span = case['span']\n",
    "        top_two = case['classification'].top_matches[:2]\n",
    "        \n",
    "        print(f\"\\nCase {i+1}: Comparing bottlenecks {top_two[0].bottleneck_id} vs {top_two[1].bottleneck_id}\")\n",
    "        print(f\"  Original scores: {top_two[0].score:.2f} vs {top_two[1].score:.2f}\")\n",
    "        \n",
    "        # Get bottleneck definitions\n",
    "        b1 = registry.get_bottleneck(top_two[0].bottleneck_id)\n",
    "        b2 = registry.get_bottleneck(top_two[1].bottleneck_id)\n",
    "        \n",
    "        # Run comparative classification\n",
    "        winner = comparative_classifier.compare_top_matches(span, b1, b2)\n",
    "        print(f\"  Winner: Bottleneck {winner}\")\n",
    "else:\n",
    "    print(\"No ambiguous cases found in this batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4107dc97-850a-4ae5-a09f-392d8f2db00e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stage 4: Causality Validation\n",
    "\n",
    "Validate that evidence shows actual causation, not just correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5f38394-bf1b-4632-b2cc-0a491e86c750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validate causality for top matches\n",
    "validated_results = []\n",
    "\n",
    "\n",
    "for result in classification_results[:5]:\n",
    "    if result['classification'].top_matches:\n",
    "        span = result['span']\n",
    "        top_match = result['classification'].top_matches[0]\n",
    "        \n",
    "        print(f\"\\nValidating evidence for bottleneck {top_match.bottleneck_id}...\")\n",
    "        \n",
    "        # Get surrounding context\n",
    "        context = {\n",
    "            'before': \"[]\",\n",
    "            'after': \"[]\",\n",
    "            'document_type': 'PER',\n",
    "            'section': 'Executive Summary'\n",
    "        }\n",
    "        \n",
    "        # Validate causality (claims)\n",
    "        causality_result = causality_validator.validate(span, context)\n",
    "        \n",
    "        print(f\"  Stated causation: {causality_result.is_stated_causation}\")\n",
    "        print(f\"  Inferred causation: {causality_result.is_inferred_causation}\")\n",
    "        print(f\"  Confidence: {causality_result.confidence:.2f}\")\n",
    "        print(f\"  Validity: {causality_result.is_valid}\")\n",
    "        \n",
    "        if causality_result.is_valid:\n",
    "            validated_results.append({\n",
    "                'span': span,\n",
    "                'bottleneck_id': top_match.bottleneck_id,\n",
    "                'classification': result['classification'],\n",
    "                'causality': causality_result\n",
    "            })\n",
    "\n",
    "print(f\"\\n\\nCausality Validation Summary:\")\n",
    "print(f\"  Total validated: {len(classification_results[:5])}\")\n",
    "print(f\"  Passed validation: {len(validated_results)}\")\n",
    "print(f\"  Rejection rate: {(1 - len(validated_results)/min(5, len(classification_results)))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b9d587-db42-456b-a843-0423a753d738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Stage 5: Evidence Arbitration\n",
    "\n",
    "Final quality control and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24c51263-aa3b-471b-b38f-197a6a1c6901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Arbitrate validated evidence\n",
    "final_evidence = []\n",
    "\n",
    "print(\"Running final evidence arbitration...\")\n",
    "print(\"This ensures only high-quality, specific evidence passes.\\n\")\n",
    "\n",
    "for validated in validated_results:\n",
    "    span = validated['span']\n",
    "    bottleneck_id = validated['bottleneck_id']\n",
    "    bottleneck = registry.get_bottleneck(bottleneck_id)\n",
    "    \n",
    "    print(f\"\\nArbitrating evidence for bottleneck {bottleneck_id}...\")\n",
    "    \n",
    "    # Run arbitration\n",
    "    arbitration_result = arbitrator.arbitrate(\n",
    "        span=span,\n",
    "        bottleneck=bottleneck,\n",
    "        classification_result=validated['classification'],\n",
    "        causality_result=validated['causality']\n",
    "    )\n",
    "    \n",
    "    print(f\"  Final decision: {arbitration_result.final_decision}\")\n",
    "    print(f\"  Quality score: {arbitration_result.quality_score:.2f}\")\n",
    "    print(f\"  Specificity: {arbitration_result.specificity_score:.2f}\")\n",
    "    \n",
    "    if arbitration_result.final_decision == 'ACCEPT':\n",
    "        # Create final evidence\n",
    "        evidence = FinalEvidence(\n",
    "            document_id=span.document_id,\n",
    "            document_name=span.document_name,\n",
    "            chunk_id=span.chunk_id,\n",
    "            page_number=span.page_number,\n",
    "            bottleneck_id=bottleneck_id,\n",
    "            bottleneck_name=bottleneck['name'],\n",
    "            evidence_text=span.text,\n",
    "            summary=arbitration_result.evidence_summary,\n",
    "            key_points=arbitration_result.key_points,\n",
    "            quality_score=arbitration_result.quality_score,\n",
    "            confidence=arbitration_result.quality_score,\n",
    "            is_causal=validated['causality'].is_stated_causation,\n",
    "            specificity_score=arbitration_result.specificity_score\n",
    "        )\n",
    "        final_evidence.append(evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44eee156-cd21-47cf-bf97-8b0688f750a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Summary of final evidence\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVIDENCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal final evidence pieces: {len(final_evidence)}\")\n",
    "\n",
    "if final_evidence:\n",
    "    # Group by bottleneck\n",
    "    evidence_by_bottleneck = {}\n",
    "    for ev in final_evidence:\n",
    "        if ev.bottleneck_id not in evidence_by_bottleneck:\n",
    "            evidence_by_bottleneck[ev.bottleneck_id] = []\n",
    "        evidence_by_bottleneck[ev.bottleneck_id].append(ev)\n",
    "    \n",
    "    print(\"\\nEvidence by bottleneck:\")\n",
    "    for bid, evidences in sorted(evidence_by_bottleneck.items()):\n",
    "        avg_quality = sum(e.quality_score for e in evidences) / len(evidences)\n",
    "        print(f\"  Bottleneck {bid}: {len(evidences)} pieces (avg quality: {avg_quality:.2f})\")\n",
    "    \n",
    "    # Show top evidence\n",
    "    print(\"\\nTop quality evidence:\")\n",
    "    sorted_evidence = sorted(final_evidence, key=lambda x: x.quality_score, reverse=True)\n",
    "    for i, ev in enumerate(sorted_evidence[:3]):\n",
    "        print(f\"\\n  [{i+1}] Bottleneck {ev.bottleneck_id} (Quality: {ev.quality_score:.2f})\")\n",
    "        print(f\"      Summary: {ev.summary[:150]}...\")\n",
    "        print(f\"      Document: {ev.document_name[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b28fff24-d26c-4af7-913f-0c9b071d03d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Run the complete agentic pipeline end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c2873c4-a052-49d5-95f7-5ca02195321c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Full pipeline execution function\n",
    "def run_agentic_pipeline(chunks: List[Dict], \n",
    "                         bottleneck_ids: List[str] = None,\n",
    "                         save_to_db: bool = False) -> List[FinalEvidence]:\n",
    "    \"\"\"\n",
    "    Run the complete agentic pipeline\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of document chunks\n",
    "        bottleneck_ids: Optional list of bottleneck IDs to process (None = all)\n",
    "        save_to_db: Whether to save results to database\n",
    "    \n",
    "    Returns:\n",
    "        List of final evidence pieces\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"#\"*70)\n",
    "    print(\"RUNNING FULL AGENTIC PIPELINE\")\n",
    "    print(\"#\"*70)\n",
    "    \n",
    "    # Get bottlenecks to process\n",
    "    if bottleneck_ids:\n",
    "        bottlenecks = [registry.get_bottleneck(bid) for bid in bottleneck_ids]\n",
    "    else:\n",
    "        bottlenecks = registry.get_all_bottlenecks()\n",
    "    \n",
    "    print(f\"\\nProcessing {len(chunks)} chunks\")\n",
    "    print(f\"Target bottlenecks: {[b['id'] for b in bottlenecks]}\")\n",
    "    \n",
    "    # Run pipeline\n",
    "    final_evidence = pipeline.process_documents(\n",
    "        chunks=chunks,\n",
    "        bottleneck_ids=[b['id'] for b in bottlenecks]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPipeline complete: {len(final_evidence)} final evidence pieces\")\n",
    "    \n",
    "    # Save to database if requested\n",
    "    if save_to_db and final_evidence:\n",
    "        # Convert to DataFrame\n",
    "        df_evidence = pd.DataFrame([ev.dict() for ev in final_evidence])\n",
    "        \n",
    "        # Save to Spark table\n",
    "        spark_df = spark.createDataFrame(df_evidence)\n",
    "        table_name = f\"prd_mega.sboost4.agentic_evidence_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        spark_df.write.mode('overwrite').saveAsTable(table_name)\n",
    "        print(f\"Saved to table: {table_name}\")\n",
    "    \n",
    "    return final_evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a20c61b2-ae3d-4956-abd2-9fd132a54cbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run full pipeline on sample data\n",
    "PIPELINE_SAMPLE_SIZE = 200\n",
    "sample_chunks = df_chunks.head(PIPELINE_SAMPLE_SIZE).to_dict('records')\n",
    "\n",
    "# Process specific bottlenecks or all\n",
    "TARGET_BOTTLENECKS = [\"1.1\", \"2.1\", \"3.1\", \"6.1\"]  # Or None for all\n",
    "\n",
    "final_results = run_agentic_pipeline(\n",
    "    chunks=sample_chunks,\n",
    "    bottleneck_ids=TARGET_BOTTLENECKS,\n",
    "    save_to_db=False  # Set to True for production\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6184f904-ba05-40ce-b393-ff41598cd897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a16e146-a613-4349-87ed-abf2cb1b5cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze pipeline performance metrics\n",
    "def analyze_pipeline_metrics(chunks_input: int, final_evidence: List[FinalEvidence]):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics of the agentic pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\nPIPELINE PERFORMANCE METRICS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  Input chunks: {chunks_input}\")\n",
    "    print(f\"  Final evidence: {len(final_evidence)}\")\n",
    "    print(f\"  Overall precision rate: {len(final_evidence)/chunks_input*100:.2f}%\")\n",
    "    \n",
    "    if final_evidence:\n",
    "        # Quality metrics\n",
    "        quality_scores = [ev.quality_score for ev in final_evidence]\n",
    "        specificity_scores = [ev.specificity_score for ev in final_evidence]\n",
    "        \n",
    "        print(f\"\\nQuality Metrics:\")\n",
    "        print(f\"  Average quality score: {sum(quality_scores)/len(quality_scores):.2f}\")\n",
    "        print(f\"  Min quality score: {min(quality_scores):.2f}\")\n",
    "        print(f\"  Max quality score: {max(quality_scores):.2f}\")\n",
    "        print(f\"  Average specificity: {sum(specificity_scores)/len(specificity_scores):.2f}\")\n",
    "        \n",
    "        # Causality metrics\n",
    "        causal_count = sum(1 for ev in final_evidence if ev.is_causal)\n",
    "        print(f\"\\nCausality Analysis:\")\n",
    "        print(f\"  Evidence with stated causation: {causal_count}\")\n",
    "        print(f\"  Evidence with correlation only: {len(final_evidence) - causal_count}\")\n",
    "        print(f\"  Causal percentage: {causal_count/len(final_evidence)*100:.1f}%\")\n",
    "        \n",
    "        # Bottleneck distribution\n",
    "        bottleneck_dist = {}\n",
    "        for ev in final_evidence:\n",
    "            bottleneck_dist[ev.bottleneck_id] = bottleneck_dist.get(ev.bottleneck_id, 0) + 1\n",
    "        \n",
    "        print(f\"\\nBottleneck Distribution:\")\n",
    "        for bid, count in sorted(bottleneck_dist.items()):\n",
    "            print(f\"  {bid}: {count} evidence pieces ({count/len(final_evidence)*100:.1f}%)\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_pipeline_metrics(len(sample_chunks), final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f032f29-ff37-460b-99a8-9b80ddf15c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab9792f3-ad77-4cc9-81c3-5433dd3725bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Export results to Excel for review\n",
    "if final_results:\n",
    "    # Convert to DataFrame\n",
    "    df_results = pd.DataFrame([ev.dict() for ev in final_results])\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df_results['pipeline_type'] = 'agentic'\n",
    "    df_results['processing_date'] = datetime.now()\n",
    "    \n",
    "    # Export to Excel\n",
    "    output_dir = \"/Volumes/prd_mega/sboost4/vboost4/Documents/input/Bottleneck/\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = f\"{output_dir}agentic_results_{timestamp}.xlsx\"\n",
    "    \n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        # Main results\n",
    "        df_results.to_excel(writer, sheet_name='Final Evidence', index=False)\n",
    "        \n",
    "        # Summary by bottleneck\n",
    "        summary_df = df_results.groupby('bottleneck_id').agg({\n",
    "            'evidence_text': 'count',\n",
    "            'quality_score': 'mean',\n",
    "            'specificity_score': 'mean',\n",
    "            'is_causal': 'sum'\n",
    "        }).round(2)\n",
    "        summary_df.columns = ['count', 'avg_quality', 'avg_specificity', 'causal_count']\n",
    "        summary_df.to_excel(writer, sheet_name='Summary')\n",
    "        \n",
    "        # Top evidence by quality\n",
    "        top_evidence = df_results.nlargest(20, 'quality_score')[[\n",
    "            'bottleneck_id', 'summary', 'quality_score', 'specificity_score', 'document_name'\n",
    "        ]]\n",
    "        top_evidence.to_excel(writer, sheet_name='Top Evidence', index=False)\n",
    "    \n",
    "    print(f\"\\nResults exported to: {output_file}\")\n",
    "    print(f\"  Total evidence: {len(df_results)} pieces\")\n",
    "    print(f\"  Bottlenecks covered: {df_results['bottleneck_id'].nunique()}\")\n",
    "else:\n",
    "    print(\"No results to export\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
