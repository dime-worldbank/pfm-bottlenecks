{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23c3247f-8147-45cf-9106-c3bc8d31d3e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from service import Service\n",
    "load_dotenv()\n",
    "from typing import List, Literal, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "import math\n",
    "from typing import Dict, Any, List\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from service import Service\n",
    "from consts import DEFAULT_LLM_MODEL\n",
    "service = Service()\n",
    "model_name = DEFAULT_LLM_MODEL\n",
    "\n",
    "\n",
    "system_prompt = '''You are a public financial management (PFM) diagnostic analyst.  \n",
    "Your job is to identify whether a passage provides evidence for Bottleneck 6.1:  \n",
    "“Ad hoc, political, and fragmented funding channels contribute to ineffective and inefficient delivery.”\n",
    "\n",
    "Judge only what is in the text.  \n",
    "1️. Detect **structural fragmentation** — multiple or parallel funding/management channels, off-budget or ad-hoc allocations, donor vs government systems, mid-year or discretionary funding, different rules or processes, etc.  \n",
    "2. Detect **delivery failure or inefficiency** — unpredictability, duplication, coordination failure, arrears, start-stop projects, or high administrative costs.  \n",
    "3️. Apply PFM knowledge: fragmentation is a *diagnostic feature* that normally implies inefficiency, but mark evidence only when cues are clear and quotable.  \n",
    "4️. Follow the output schema exactly; quote trigger spans from the text; abstain if uncertain.\n",
    "\n",
    "Be precise, conservative, and evidence-based.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7332f2b4-d33b-4ea2-b101-083216d98506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('bottleneck_final_summarized_for_review.xlsx', sheet_name='6.1')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db6e2830-ca50-44e4-be31-ac5e80464552",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['Review Result (Relevant, Irrelevant)'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b4b9296-1a5d-4014-8742-0f4ea013776e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('bottleneck_final_summarized_for_review.xlsx', sheet_name='6.1')\n",
    "df = df[df['Review Result (Relevant, Irrelevant)'].notna()]\n",
    "rel_cols = ['node_id','chunk_id', 'extended_context','extracted_evidence', 'Review Result (Relevant, Irrelevant)',\n",
    "            'Reason for irrelevant or unsure ']\n",
    "df = df[rel_cols]\n",
    "df = df[df['Review Result (Relevant, Irrelevant)'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85afd731-2ada-47bb-bb9d-a65bb061a6d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "challenge = '''Unreliable, delayed and fragmented funding for delivery'''\n",
    "bottleneck = '''6.1 Ad hoc, political and fragmented funding channels contribute to ineffective and inefficient delivery'''\n",
    "extended_definition = '''\n",
    "Extended definition: The funding for service delivery and projects might be fragmented due to the existence of different funding sources or channels, such as budget general funds (general non-earmarked funding from the budget) from different ministries/agencies, earmarked funds from transfers coming from other governments, institutions and organizations and from earmarked government revenues, as well as funding coming from donors. Dealing with multiple funding can be challenging, especially if there is not an integrated and consolidated planning and management of such funding (these funds being managed with their corresponding parallel systems). Different management stages and procedures, non-consolidated information about the expected, distributed and consumed funds, as well as different levels of discretion in regard the use of the funds can make difficult a rational policy planning (due to uncertainty and lack of predictability of funding) and implementation, in addition to a more costly administration and higher risks of corruption and funds diversion.     \n",
    "\n",
    "Political discretion on the management of specific funds may create relevant obstacles to adequate planning and execution of service delivery and projects, as it may imply significant levels of uncertainty in regard to the allocated resources. Changing priorities, biases towards new projects and other elements may reduce the predictability and effective availability of funding for specific services and projects, making it difficult for agencies and departments to plan effectively and sustain long-term projects. This volatile nature of funding can lead to start-stop patterns in project execution, for example, where projects are initiated but then stalled due to sudden withdrawals or delays in funding.   \n",
    "\n",
    "The atomization and fragmentation of funding, with different parallel management (with their own stages and processes), reporting mechanisms and information systems contributes to administrative burdens and inefficient delivery, as well as making more difficult ensure a sound coordination, planning and execution of projects. These parallel systems may exist, for example, because of donors' contributions to the financing of specific services or projects, in order to fulfill their own financial management and control and reporting requirements, but at the cost of hindering client government’s systems and the integrated management of funds and service and project planning and coordination.  For example, donor’s use of Ethiopia’s public financial management and procurement systems significantly declined between the middle and end of the decade of 2010, with more than 50% of donor funding routed through parallel systems , creating complexity and lack of integration of services funding. Alike, in Uganda and Indonesia, the existence of multiple fragmented sources of financing created major challenges for local government planning, budget formulation and execution in regard of education policies.   \n",
    "\n",
    "In addition, the effects of the fragmentation of the different funding channels can foster, and be amplified, by the lack of adequate coordination between sectors, levels of government and partners, resulting in a duplication of efforts, and inefficiencies on the allocation, channeling and execution of resources.  For example, different agencies might end up competing for the same funds or, conversely, some funds might remain underutilized due to lack of awareness or coordination among potential beneficiaries, or, for policies planned and executed across different agencies and levels of government, poor coordination may end up with an inefficient policy development and allocation of resources (not coordinated planned actions and prepared budgets towards common/shared policy goals), as well as with delays and/or shortages in regard the distribution of funds. For instance, in Rwanda108, the funding and the implementation of gender-based violence policies managed by many agencies across the government (at least four ministries) and district governments, largely relying on donors funding, and with non-public funded services provided by outside of government institutions, organizations and communities, resulting challenging in terms of policy integration and optimal management of resources. Particularly relevant and frequent might be the case of fragmented and uncoordinated different funding mechanisms across levels of governments. In Nigeria health financing streams were highly fragmented across and within levels of government, with health facilities receiving both federal and state resources, contributory health insurance schemes being managed by separate small pools, with the Basic Healthcare Provision Fund creates even more funding pools managed by state and insurance agencies, setting up a very complex system between different levels of governments and operators. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a235fb59-6069-4666-83b8-1110a8628176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a PFM diagnostic coder. Your task is to judge whether the INPUT TEXT\n",
    "contains evidence for a Bottleneck classified under the larger topic: {challenge}.\n",
    "\n",
    "The specific bottleneck is defined as: {bottleneck}.\n",
    "\n",
    "An extended definition with more context is given below:\n",
    "\n",
    "{extended_definition}\n",
    "\n",
    "Output MUST follow the JSON schema you are given (no extra keys, no prose).\n",
    "\n",
    "––––– SCOPE LOCK (IMPORTANT) –––––\n",
    "Treat fragmentation strictly as **funding/financial-management channel fragmentation**.\n",
    "Program/administrative separation is irrelevant **unless** it creates a **distinct funding flow or a\n",
    "parallel financial control stream** (e.g., separate accounts, off-budget handling, different release/disbursement rules).\n",
    "\n",
    "––––– DEFINITIONS (OPERABLE) –––––\n",
    "• Fragmentation cues (diagnostic features) — funding/finance anchored:\n",
    "  STRONG (any ONE is sufficient for feature-based evidence):\n",
    "    - off-budget flows (outside the main budget/TSA/IFMIS); parallel accounts/systems\n",
    "    - donor vs government **follow different rules/processes** (allocation/release/procurement/reporting)\n",
    "    - political/ad-hoc channels bypassing normal process (e.g., block/discretionary funds, mid-year approvals outside ADP/MTEF)\n",
    "    - multiple **financing pools** for the same service managed by different agencies\n",
    "    - explicit bypass of government systems; pooled fund dissolved → parallel handling\n",
    "  MODERATE (need TWO together if no STRONG):\n",
    "    - earmarked/tied funds with separate handling\n",
    "    - vertical (intergovernmental) fragmentation with **distinct pools** (federal/state/insurer)\n",
    "    - procedural divergence across **funding streams** (release/reporting/procurement/IFMIS)\n",
    "    - volatility explicitly revealing separate handling (e.g., “moved off-budget”, “bypassed TSA”)\n",
    "\n",
    "• Delivery failure / inefficiency (consequences):\n",
    "  - higher administrative costs/inefficiency/duplication\n",
    "  - unpredictability/volatility in funding; start–stop projects\n",
    "  - coordination failures (duplication, underutilization, competition for funds)\n",
    "  - delays/arrears/cash shortfalls\n",
    "  - corruption/leakage risk\n",
    "\n",
    "––––– HARD NEGATIVES (EXCLUDE) –––––\n",
    "If the text is ONLY about any of the following (with no qualifying funding-channel cue), set hard_negative=true and decision=\"irrelevant\":\n",
    "  - wage-bill/personnel/payroll control (PSC, hiring discretion, casual workers, **separate budget vote/line**) **on-budget**\n",
    "  - generic program-budget/accountability structure (e.g., “draws from multiple subprograms”) without distinct funding channels/systems\n",
    "  - capacity/procurement/audit findings without alternate channels\n",
    "  - revenue politics (divisible pool shares, opposing OSR reforms) **without** off-budget/ad-hoc fees/parallel accounts\n",
    "  - donor dependence/withdrawal **without** off-budget/parallel handling or different rules\n",
    "\n",
    "––––– TRIGGER SPAN REQUIREMENT –––––\n",
    "Before labeling Relevant, extract verbatim **finance trigger spans** from the INPUT TEXT (not the context) that reference money-flow/control terms such as:\n",
    "  “off-budget”, “TSA”, “IFMIS”, “parallel account/system”, “pooled fund/pool”, “allocation”, “release”, “warrant”,\n",
    "  “disbursement”, “on-budget”, “block/discretionary fund”, “mid-year approval”, “ADP/MTEF”, “donor-funded vs government-funded (different rules)”.\n",
    "\n",
    "––––– ACCEPTANCE RULES –––––\n",
    "1) is_fragmentation_evidence = true  iff\n",
    "   ((STRONG >= 1) OR (MODERATE >= 2)) AND trigger_spans (finance words) nonempty AND hard_negative = false.\n",
    "2) is_fragmentation_plus_failure = true  iff\n",
    "   is_fragmentation_evidence = true AND failure_present = true.\n",
    "3) If no explicit **finance** phrase can be quoted for trigger_spans, set decision to \"abstain\" (even if you suspect relevance).\n",
    "4) Always fill trigger_spans and failure_spans with verbatim phrases from the INPUT TEXT that justify your labels.\n",
    "\n",
    "––––– OUTPUT FORMAT –––––\n",
    "Return ONLY JSON that conforms to the schema provided below. Do not include commentary.\n",
    "\n",
    "––––– INPUT TEXT –––––\n",
    "{input_text}\n",
    "\n",
    "This input text is extracted from the following extended source context. Use it only for corroboration (do not quote from it):\n",
    "{extended_context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "StrongCue = Literal[\n",
    "    \"off_budget\",\n",
    "    \"parallel_systems\",\n",
    "    \"separate_administration\",\n",
    "    \"different_rules_processes_across_streams\",\n",
    "    \"political_ad_hoc_allocation\",\n",
    "    \"mid_year_approval_outside_cycle\",\n",
    "    \"block_allocation_outside_process\",\n",
    "    \"multiple_financing_pools_different_agencies\",\n",
    "]\n",
    "\n",
    "ModerateCue = Literal[\n",
    "    \"earmarked_or_tied_grants\",\n",
    "    \"vertical_fragmentation_intergovernmental\",\n",
    "    \"procedural_divergence_reporting_procurement_ifmis\",\n",
    "    \"pooled_fund_dissolved_or_bypass\",\n",
    "    \"volatility_revealing_fragmentation\",\n",
    "]\n",
    "\n",
    "FailureType = Literal[\n",
    "    \"inefficiency_higher_admin_costs\",\n",
    "    \"unpredictability_funding_volatility\",\n",
    "    \"coordination_failure_duplication_underutilization_competition\",\n",
    "    \"delays_start_stop\",\n",
    "    \"arrears_or_cash_shortfalls\",\n",
    "    \"corruption_or_leakage_risk\",\n",
    "]\n",
    "\n",
    "Subtype = Literal[\n",
    "    \"donor_gov_parallel\",\n",
    "    \"domestic_vs_donor_different_rules\",\n",
    "    \"political_ad_hoc_intragovernment\",\n",
    "    \"vertical_intergovernmental_fragmentation\",\n",
    "    \"program_administration_fragmentation\",\n",
    "    \"other\",\n",
    "]\n",
    "\n",
    "Decision = Literal[\n",
    "    \"relevant_feature\",                 # feature-based evidence satisfied\n",
    "    \"relevant_feature_and_failure\",     # fragmentation + failure satisfied\n",
    "    \"irrelevant\",                       # hard negative or no cues\n",
    "    \"abstain\",                          # insufficient quotable spans or uncertain\n",
    "]\n",
    "\n",
    "class BottleneckEvidence(BaseModel):\n",
    "    \"\"\"\n",
    "    Structured judgment for PFM Bottleneck 6.1.\n",
    "    Follows precision-first rules: strong/moderate cues, hard negatives, and span extraction.\n",
    "    \"\"\"\n",
    "    # Primary decisions\n",
    "    is_fragmentation_evidence: bool = Field(\n",
    "        ..., description=\"Feature-based verdict (diagnostic). True iff STRONG>=1 or MODERATE>=2 and trigger_spans present and not hard_negative.\"\n",
    "    )\n",
    "    is_fragmentation_plus_failure: bool = Field(\n",
    "        ..., description=\"Consequence-based verdict. True iff is_fragmentation_evidence and failure_present.\"\n",
    "    )\n",
    "    decision: Decision = Field(\n",
    "        ..., description='Overall decision among {\"relevant_feature\",\"relevant_feature_and_failure\",\"irrelevant\",\"abstain\"}.'\n",
    "    )\n",
    "\n",
    "    # Evidence for fragmentation\n",
    "    strong_cues: List[StrongCue] = Field(default_factory=list, description=\"Which strong cues were found.\")\n",
    "    moderate_cues: List[ModerateCue] = Field(default_factory=list, description=\"Which moderate cues were found.\")\n",
    "    hard_negative: bool = Field(False, description=\"True if the passage matches any exclusion rule.\")\n",
    "    trigger_spans: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Verbatim phrases from the text that support fragmentation cues (must be substrings of the input).\",\n",
    "    )\n",
    "    subtype: Subtype = Field(\"other\", description=\"Subtype of fragmentation, if any.\")\n",
    "\n",
    "    # Evidence for failure/inefficiency\n",
    "    failure_present: bool = Field(False, description=\"True if explicit or clearly implied delivery failure/inefficiency is present.\")\n",
    "    failure_types: List[FailureType] = Field(default_factory=list, description=\"Categorization of the failure/inefficiency mentioned.\")\n",
    "    failure_spans: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Verbatim phrases supporting failure/inefficiency (must be substrings of the input).\",\n",
    "    )\n",
    "\n",
    "    rationale: str = Field(\n",
    "        ...,\n",
    "        description=\"2-3 sentence justification referencing the trigger_spans/failure_spans; avoid speculation; no new facts.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8626a0c9-78c7-4853-8685-bd36de7367ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#node_id, chunk_id, extended_context, extracted_evidence, expert_classification, reason_for_expert_irrelevant = df.sample().iloc[0].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f7765ff-0079-49bb-a216-c77b7034f6ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af2d4f1b-1359-49a6-a8ba-6e629b3dfa54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1114144-1ee4-480f-870b-1199d5ae45bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start, end = 150, df.shape[0]\n",
    "\n",
    "for item in tqdm(df.values.tolist()[start:end]):\n",
    "    node_id, chunk_id, extended_context, extracted_evidence, expert_classification, reason_for_expert_irrelevant = item\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        challenge = challenge,\n",
    "        bottleneck = bottleneck,\n",
    "        extended_definition=extended_definition,\n",
    "        input_text=extracted_evidence,\n",
    "        extended_context=extended_context,\n",
    "    )\n",
    "    \n",
    "    output = service.execute(\n",
    "        prompt=prompt,\n",
    "        model=DEFAULT_LLM_MODEL,\n",
    "        response_model=BottleneckEvidence,\n",
    "        temperature=0.0,\n",
    "        system_message=system_prompt\n",
    "    )\n",
    "    l.append((node_id, chunk_id, extended_context, extracted_evidence, expert_classification, reason_for_expert_irrelevant, output, ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34f94c51-f09a-4ba9-9016-75a4a6830566",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def relevance_boolean(x):\n",
    "    return x=='Relevant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bff4c69-68c0-4c79-b0ea-99fcb1d00c44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = [(relevance_boolean(item[-3]), item[-1].is_fragmentation_evidence, item[-1].is_fragmentation_plus_failure) for item in l]\n",
    "df_results = pd.DataFrame(m, columns = ['expert_label', 'model1_label', 'model2_label'])\n",
    "precision_m1 = (\n",
    "    df_results.query(\"model1_label == True and expert_label == True\").shape[0]\n",
    "    / df_results.query(\"model1_label == True\").shape[0]\n",
    "    if df_results.query(\"model1_label == True\").shape[0] > 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "precision_m2 = (\n",
    "    df_results.query(\"model2_label == True and expert_label == True\").shape[0]\n",
    "    / df_results.query(\"model2_label == True\").shape[0]\n",
    "    if df_results.query(\"model2_label == True\").shape[0] > 0\n",
    "    else 0\n",
    ")\n",
    "\n",
    "print(f\"Precision (Model 1): {precision_m1:.2f}\")\n",
    "print(f\"Precision (Model 2): {precision_m2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd90139-476e-427a-8ec4-1a6bdcc9cee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['model1_label'] = df_results.model1_label.values.tolist()\n",
    "df['model2_label'] = df_results.model2_label.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2e70b70-ef63-4529-869c-20273cf0f3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6dc2d15-318c-4d16-9726-a64a5dd844ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('6_1_revised_extarction_validation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db3c7674-974a-4035-9f0e-51c57745a31b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24cbddd9-8c69-4431-823a-fd072b93a1e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e29b8b-6395-4f74-834b-5909844c22ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run on the prefiltered list of 600 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a98ac9c-8244-44bb-9a36-ee307fe7c214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa12ca18-eb72-4d7a-b292-f38f401e5556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_chunks = pd.read_csv('per_pfr_chunks_with_prefilter_results.csv')\n",
    "df_rel_chunks = df_chunks[df_chunks.prefilter_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65883f0-cdb6-40e2-a652-db242e276e01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The prefliter was done on the full chunk so we dont have the associated extracted evidence text. We first get this potential evidence\n",
    "class ConfidenceLevel(str, Enum):\n",
    "    strong = \"strong\"\n",
    "    borderline = \"borderline\"\n",
    "    weak = \"weak\"\n",
    "\n",
    "class BottleneckBase(BaseModel):\n",
    "    confidence: Optional[ConfidenceLevel] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"How confidently the extracted evidence supports the bottleneck. \"\n",
    "            \"Choose 'strong' if the evidence clearly and directly supports the bottleneck, \"\n",
    "            \"'borderline' if it is somewhat relevant but may be open to interpretation, \"\n",
    "            \"and 'weak' if the evidence is tenuous, ambiguous, or only indirectly related.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "class Bottleneck_2_1(BottleneckBase):\n",
    "    extracted_evidence: Optional[str] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Verbatim excerpt from the text that provides concrete evidence of fragmented, inconsistent, or uncoordinated policy design. \"\n",
    "            \"Look for examples of conflicting mandates, duplicative schemes, lack of alignment across sectors or institutions, or absence of cross-sector coordination mechanisms. \"\n",
    "            \"Do not extract vague critiques of policy or general governance weakness without explicit reference to inter-policy inconsistency or siloed formulation. \"\n",
    "            \"Use only direct text from the source; do not paraphrase or infer.\"\n",
    "        )\n",
    "    )\n",
    "    reasoning: Optional[str] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Brief explanation of how the extracted text illustrates fragmented or uncoordinated policy design. \"\n",
    "            \"The reasoning should clarify why the excerpt demonstrates lack of alignment or duplication, and avoid interpretation beyond the quoted material.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "def make_bottleneck_prompt(text: str) -> str:\n",
    "\n",
    "    return f\"\"\"\n",
    "        You are analyzing a public finance document to identify specific bottlenecks affecting development outcomes.\n",
    "        \n",
    "        The context for your analysis is as follows:\n",
    "        \n",
    "        Role of Public Finance: Effective Resource Mobilization & Distribution\n",
    "        role_description: Governments need to raise and allocate and influence private financial resources in support of the pursuit of their policy objectives, ensuring both that sufficient resources are available when needed, and that these are allocated according to needs and cost-effectiveness criteria. How governments do this has important distributional impacts and can influence public and private behaviour towards achievement of objectives.\n",
    "        \n",
    "        PFM Challenge: Unreliable, delayed and fragmented funding for delivery\n",
    "        → challenge_description: Assesses how predictable, timely, and well-coordinated funding flows are, and whether fragmentation or delays impede delivery\n",
    "        \n",
    "        Specific Bottleneck: Ad hoc, Political and Fragmented Funding Channels\n",
    "        → bottleneck description: Governments and public sector entities often rely on multiple, uncoordinated funding mechanisms—such as general funds, earmarked revenues, donor funding, and intergovernmental transfers. These mechanisms often lack integration, leading to volatility, administrative duplication, and fragmented service delivery. Discretionary or politically influenced allocation, parallel management systems (especially from donors), and poor coordination across agencies or levels of government compound inefficiencies. Common issues include delays, incomplete disbursements, excessive reporting burdens, and poor alignment of funding with long-term plans or shared objectives.\n",
    "        \n",
    "        ---\n",
    "        \n",
    "        Your task:\n",
    "        \n",
    "        - Carefully read the excerpt below.\n",
    "        - Extract direct evidence from the text that clearly supports the presence of the specific bottleneck listed above.\n",
    "        - Only extract text that is explicitly present in the excerpt.\n",
    "        - Do not infer, assume, or include information that is not stated.\n",
    "        - If you find no clear evidence, return null.\n",
    "        \n",
    "        For each piece of extracted evidence, briefly explain your reasoning (i.e., why this excerpt indicates the bottleneck), and indicate if the match is ambiguous.\n",
    "        \n",
    "        Text to analyze:\n",
    "        \n",
    "        {text}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2203771b-3a8e-4006-86b3-3c929f34b37f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = '''You are a public finance expert working with a multilateral development institution.\n",
    "\n",
    "    Your task is to carefully read and analyze Public Finance Review (PFR) documents, budget support documents, or other fiscal diagnostics and project reports produced by institutions such as the World Bank or IMF.\n",
    "\n",
    "    You are trained to identify and extract supporting evidence for specific Public Financial Management (PFM) bottlenecks, based on a predefined set of challenges and bottlenecks.\n",
    "\n",
    "    The evidence may appear in explicit or implicit form. You should be attentive to:\n",
    "    - Descriptions of institutional weakness or fragmentation\n",
    "    - Observations about policy design vs implementation\n",
    "    - Statements about fiscal sustainability, resource adequacy, or funding flows\n",
    "    - Structural, capacity-related, or political-economy constraints\n",
    "    - Trends, examples, or observations—both qualitative and quantitative\n",
    "\n",
    "    You will return evidence *only if it clearly supports the described bottleneck*, otherwise leave it blank.\n",
    "\n",
    "    Be concise but specific. Use quotes from the document when possible or summarize tightly if quoting is impractical.\n",
    "    '''\n",
    "l = []\n",
    "\n",
    "for item in tqdm(df_rel_chunks.iterrows()):\n",
    "    node_id, chunk_id, text, _ = item[1]\n",
    "    prompt = make_bottleneck_prompt(text=text)\n",
    "    result = service.execute(\n",
    "        prompt=prompt,\n",
    "        model=DEFAULT_LLM_MODEL,\n",
    "        response_model=Bottleneck_2_1,\n",
    "        temperature=0.0,\n",
    "        system_message=DEFAULT_SYSTEM_PROMPT\n",
    "    )\n",
    "    if result.extracted_evidence:\n",
    "        l.append(item[1].values.tolist()+[result.extracted_evidence])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d6b2c05-f192-47b7-858e-f78f9525985c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols =  ['node_id', 'chunk_id', 'chunk', 'prefilter_tag', 'extracted_evidence']\n",
    "ddf = pd.DataFrame(l, columns =cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef59f17b-2b7c-424a-844c-e9e520813af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c7a2fbc-d0a8-40aa-a20a-1ceb46eb61d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d18cc78-6b19-4422-9c4f-09429f70cd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# run the revised validation model on these chunks\n",
    "\n",
    "start, end = 300, ddf.shape[0]\n",
    "\n",
    "for item in tqdm(ddf.values.tolist()[start:end]):\n",
    "    node_id, chunk_id, extended_context, _, extracted_evidence = item\n",
    "    prompt = PROMPT_TEMPLATE.format(\n",
    "        challenge = challenge,\n",
    "        bottleneck = bottleneck,\n",
    "        extended_definition=extended_definition,\n",
    "        input_text=extracted_evidence,\n",
    "        extended_context=extended_context,\n",
    "    )\n",
    "    \n",
    "    output = service.execute(\n",
    "        prompt=prompt,\n",
    "        model=DEFAULT_LLM_MODEL,\n",
    "        response_model=BottleneckEvidence,\n",
    "        temperature=0.0,\n",
    "        system_message=system_prompt\n",
    "    )\n",
    "    new_output.append((node_id, chunk_id, extended_context, extracted_evidence, output))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17aa6096-ce77-441c-904a-5cef2eb2dca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf['model1_label'] = [item[-1].is_fragmentation_evidence for item in new_output]\n",
    "ddf['model2_label'] = [item[-1].is_fragmentation_plus_failure for item in new_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27df535d-88f4-434a-8279-37e50a3221d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771b0639-d25f-46cb-a249-cf56085f1962",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf.to_csv('prefiltered_6_1_extraction_validation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f694385a-19e4-4a34-be6f-a204ee778de7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf.model1_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa767840-216e-42db-a83d-83b9c078069d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43b430a9-0cc2-4838-9700-00fa1bc015e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "384b9c21-d92b-4735-bb35-6ba5fc450770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969f76ae-5a12-4602-931b-254b2d24186d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a1cf41-5bbb-4c53-aaca-7e7602a7d48b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf=pd.read_csv('prefiltered_6_1_extraction_validation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c66602c-f176-4eb5-9d42-3081b7d7e948",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_chunks = pd.read_csv('per_pfr_chunks.csv')\n",
    "df_docs =  pd.read_csv('per_pfr_document_data.csv')\n",
    "d_chunks = {(x[0], x[1]):x[2] for x in df_chunks.values.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd817f27-136c-4e9b-a9d6-7eab7210427a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_extended_context(node_id, chunk_id, n=2):\n",
    "    return '\\n\\n'.join([d_chunks.get((node_id, chunk_id+i), '') for i in [-2, -1, 0, 1, 2]])\n",
    "\n",
    "def get_metadata(node_id):\n",
    "    try:\n",
    "        return df_docs[df_docs.node_id==node_id][['cntry_name', 'doc_name', 'admin_rgn_name', 'ent_topic_text']].to_records().tolist()[0][1:]\n",
    "    except:\n",
    "        return ('', '', '', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03c64d51-7fab-4a42-a4fd-c4ae8c7fd18d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_metadata(5669851)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40c2bfbd-ed48-4703-919c-6efd68af0ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cfb8fc0-bfd7-4ceb-b626-38c2096c0e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf['extended_context'] = ddf.apply(lambda x: get_extended_context(x['node_id'], x['chunk_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867d0c07-510c-44b9-a75a-20e6bc573198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac81084-e488-45b4-8616-cc214cb23ad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"Student loan repayment rates in Lesotho and Tanzania: There is an announced policy on loan repayment but in practice repayments are not collected with much effort or consistency, so a larger share of public financing for education goes to post-secondary education than the policy requires.\",\n",
    "    \"Liberia: Budget credibility remains weak. Liberia has struggled to implement budgets as planned – aggregate expenditure outturns have significantly deviated from approved budgets, indicating limited credibility. Public spending has been highly volatile (rising sharply during aid-fueled booms and then contracting), undermining a stable counter-cyclical policy.\",\n",
    "    \"Political commitment to gender equality: According to a recent survey of 12 developing countries, almost all policy-makers (96%) state that 'schools should promote gender equality' yet at the same time almost half (47%) also believe that 'mothers working is bad for their children'. This demonstrates the often large gap between policy rhetoric and policy commitment regarding whether girls should have the same opportunities as boys.\",\n",
    "    \"UNFPA estimate that $42 billion of investment is needed to end gender-based violence in 132 priority countries by 2030, of which only $9bn has been identified, leaving a funding gap of $32.5 billion.\",\n",
    "    \"Despite evidence that primary health care is the most cost-effective and equitable way to make progress towards UHC, government health spending devoted to primary health care is low across all income groups: 33% in LICs, 36% in LMICs, 34% in UMICs and 36% in HICs (Hanson et al., 2022).\",\n",
    "    \"Albania projected a 17% funding gap against its GBV strategy costs.\",\n",
    "    \"In Belgium, cuts in the federal budget had resulted in disparities in the GBV policies issued at the regional level, as well as the reduction of funding of the voluntary sector.\"\n",
    "]\n",
    "\n",
    "bottleneck_name = 'Inadequate Commitment of Political and Technical Leadership'\n",
    "bottleneck_description = (\n",
    "                    \"This bottleneck applies when there is a clear lack of sustained commitment by political or technical leaders to implement approved policies. \"\n",
    "                    \"This includes delays, resistance, or failure to act when reforms threaten the status quo, require politically difficult trade-offs, \"\n",
    "                    \"or demand resource shifts that are not followed through despite stated priorities. \"\n",
    "                    \"Examples include: approved reforms not being enacted, persistent underfunding of a priority despite commitments, or misalignment between stated goals and actual budget execution. \"\n",
    "                    \"Do **not** classify general governance weakness, vague statements, or budget/funding gaps **unless** directly tied to political/technical unwillingness or inaction. \"\n",
    "                    \"Be careful to distinguish from other bottlenecks like 2.1 (coordination failures), 5.2 (disconnect between budgets and policy), or 6.3 (weak execution).\"\n",
    "                )\n",
    "\n",
    "\n",
    "class StructuredSummaryFields(BaseModel):\n",
    "    country: str = Field(..., description=\"Country referenced in the extarcted evidence.\")\n",
    "    issue_area: Optional[str] = Field(None, description=\"Main sector or topic affected by the bottleneck.\")\n",
    "    reference_to_policy_or_program: Optional[str] = Field(None, description=\"Any specific or general policy referenced in the context of the extracted evidence.\")\n",
    "    reference_outcome: Optional[str] = Field(None, description=\"What development or sectoral outcome is affected by the lack of leadership commitment? This could include unmet policy goals, reduced service delivery, poor implementation, or failure to achieve intended change. This is from the reference extracted eivdence for the bottleneck\")\n",
    "    key_constraint: str = Field(..., description=\"Describe what constraint is being evidenced.\")\n",
    "    observed_consequence: Optional[str] = Field(None, description=\"Any outcome or effect of the constraint.\")\n",
    "    metric_or_statistic: Optional[str] = Field(None, description=\"Quantitative detail if present (e.g., a funding gap, percentage).\")\n",
    "    closest_sdg: Optional[str] = Field(None, description=\"If directly relevant, mention the closest Sustainable Development Goal (SDG). Leave blank if nothing is directly relevant. IF present and relevant then report this in the form  in this example format'SDG 16: Peace, Justice and Strong Institutions'\")\n",
    "    closest_sdg_target: Optional[str] = Field(None, description=\"If directly relevant, mention the closest Sustainable Development Goal (SDG) target. Leave blank if nothing is directly relevant. If relevant use the format in the following example: 'Eliminate all harmful practices, such as child, early and forced marriage and female genital mutilation' \")\n",
    "\n",
    "class StylizedSummary(BaseModel):\n",
    "    summary_text: str = Field(..., description=\"A short, stylized summary suitable for inclusion in a public finance report.\")\n",
    "\n",
    "\n",
    "def make_structured_summary_prompt(\n",
    "    context_text: str,\n",
    "    extracted_evidence: str,\n",
    "    bottleneck_name: str,\n",
    "    bottleneck_description: str,\n",
    "    metadata_tuple: tuple,\n",
    "    examples: list[str]\n",
    ") -> str:\n",
    "\n",
    "    country, doc_name, region, topic_text = metadata_tuple\n",
    "    example_block = \"\\n\".join(f\"- {ex}\" for ex in examples)\n",
    "\n",
    "    return f\"\"\"\n",
    "    You are a public finance expert working at a multilateral development bank.\n",
    "\n",
    "    Your task is to extract structured components that support stylized, policy-relevant summaries\n",
    "    of public financial management (PFM) bottlenecks.\n",
    "\n",
    "    You are given:\n",
    "    - Context text from a fiscal diagnostic report\n",
    "    - A validated excerpt from that context which supports a known PFM bottleneck referred to as the extracted evidence\n",
    "    - Document metadata (e.g., country, region, topics)\n",
    "    - A bottleneck definition (e.g., inadequate leadership commitment, funding fragmentation)\n",
    "\n",
    "    ---\n",
    "\n",
    "    Document:\n",
    "    - Country: {country}\n",
    "    - Title of document: {doc_name}\n",
    "    - Region: {region}\n",
    "    - Topics: {topic_text}\n",
    "\n",
    "    Bottleneck:\n",
    "    **{bottleneck_name}** → {bottleneck_description}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Here is the full context from the document:\n",
    "    {context_text}\n",
    "\n",
    "    Here is the specific quote that was validated as evidence (extracted evidence) for the bottleneck titled {bottleneck_name}:\n",
    "    \\\"\\\"\\\"{extracted_evidence}\\\"\\\"\\\"\n",
    "\n",
    "    ---\n",
    "\n",
    "    Your task is to extract the following structured fields:\n",
    "    - Country\n",
    "    - Issue Area (e.g., health, education, fiscal management, GBV, etc.)\n",
    "    - Reference to policy or program (if applicable)\n",
    "    - Key constraint (explain what’s blocking implementation or delivery)\n",
    "    - Observed consequence (if mentioned)\n",
    "    - Metric or statistic (if present)\n",
    "\n",
    "    Only extract what is clearly grounded in the text. Do not invent or infer missing details.\n",
    "\n",
    "    ---\n",
    "\n",
    "    To guide your understanding of the level of detail and relevance we expect, here are some example summaries that your structured fields will ultimately help produce:\n",
    "\n",
    "    {example_block}\n",
    "    \"\"\".strip()\n",
    "\n",
    "def make_summary_from_structure_prompt(\n",
    "    extracted_evidence: str,\n",
    "    structured: StructuredSummaryFields,\n",
    "    bottleneck_name: str,\n",
    "    examples: list[str]\n",
    ") -> str:\n",
    "    example_block = \"\\n\".join(f\"- {ex}\" for ex in examples)\n",
    "\n",
    "    return f\"\"\"\n",
    "You are writing a concise, report-style summary of a public finance implementation bottleneck. The main content for this summary is the extracted evidence from document chunks: {extracted_evidence}\n",
    "\n",
    "In order to frame the summary -- you can refer to information in the structured input as well as some sample examples as shown below. \n",
    "\n",
    "Match the tone following examples:\n",
    "\n",
    "{example_block}\n",
    "\n",
    "---\n",
    "\n",
    "Use the structured information below to generate a clear, concise summary (1–5 sentences).\n",
    "The summary should:\n",
    "- Mention the **country**\n",
    "- Describe the **constraint** clearly\n",
    "- Refer to the **policy or program** (if relevant)\n",
    "- Include **consequence or impact**\n",
    "- Refer to the **outcome** (if relevant)\n",
    "- Mention **quantitative figures** if present\n",
    "- Do not remove any information that is already present. Keep all existing details but structure it in form anf tone of the examples shown.\n",
    "- Do not add **extra commentary** unless this information is already present in the input information. \n",
    "- Do not explicitly include SDGs unless mentioned in the extracted evidence. \n",
    "- Do not use **any** language **suggesting causal effects and conclusions if not explicitly present** in the input context \n",
    "\n",
    "Structured input:\n",
    "{structured.model_dump_json(indent=2)}\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db54998-a5c3-4ffd-824e-8da018a3f436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ddf_selected = ddf[ddf.model2_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb667f0-7ac4-42b0-91f8-3cf5a8fd2cc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_output = []\n",
    "for item in tqdm(ddf_selected.iterrows()):\n",
    "    extracted_evidence = item[1]['extracted_evidence']\n",
    "    node_id = item[1]['node_id']\n",
    "    chunk_id = item[1]['chunk_id']\n",
    "    context_text = item[1]['extended_context']\n",
    "    metadata_tuple = get_metadata(node_id)\n",
    "\n",
    "    structured_output = service.execute(\n",
    "        prompt=make_structured_summary_prompt(\n",
    "            context_text=context_text,\n",
    "            extracted_evidence=extracted_evidence,\n",
    "            bottleneck_name=bottleneck,\n",
    "            bottleneck_description=extended_definition,\n",
    "            metadata_tuple=metadata_tuple,\n",
    "            examples=examples\n",
    "        ),\n",
    "        model=DEFAULT_LLM_MODEL,\n",
    "        response_model=StructuredSummaryFields\n",
    "    )\n",
    "\n",
    "    summary_result = service.execute(\n",
    "        prompt=make_summary_from_structure_prompt(\n",
    "            extracted_evidence=extracted_evidence,\n",
    "            structured=structured_output,\n",
    "            bottleneck_name=bottleneck_name,\n",
    "            examples=examples\n",
    "        ),\n",
    "        model=DEFAULT_LLM_MODEL,\n",
    "        response_model=StylizedSummary\n",
    "    )\n",
    "    final_summary = summary_result.summary_text\n",
    "    result  = {**{\n",
    "        'node_id': node_id,\n",
    "        'chunk_id': chunk_id, \n",
    "        'extended_context': context_text, \n",
    "        'extracted_evidence': extracted_evidence, \n",
    "        'final_summary': final_summary}, **structured_output.model_dump()}\n",
    "    final_output.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dcc5dc0-689e-4180-bea9-27f89ecae6ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68751567-93ac-4d6f-9e6e-33c4dadfa6b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final.to_csv('prefiltered_6_1_extraction_validation_results_with_summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8868fba7-03d4-4e60-8f35-929b081af3fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "6_1ad_hoc_funding_extraction_validation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Witchcraft Cases",
   "language": "python",
   "name": "witchcraft"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
