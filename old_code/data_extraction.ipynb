{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b8f16b-5c7a-4573-9b3b-20dac8e1d876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25352851-a4ef-4f13-a874-f4040662abf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DataType\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26346f68-232b-4a7c-8ff2-37d79848981a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM prd_corpdata.dm_reference_gold.v_dim_imagebank_document\n",
    "WHERE (\n",
    "    (doc_name ILIKE '%Public expenditure review%' OR\n",
    "    doc_name ILIKE '%Public finance review%') AND\n",
    "    --doc_name ILIKE '%(PER)%' OR\n",
    "    --doc_name ILIKE '%(PFR)%'\n",
    "    doc_type_name in ('Report', 'Public Expenditure Review') AND\n",
    "    doc_name not ILIKE '%Summary%' AND\n",
    "    doc_name not ILIKE '%Synthesis%' AND\n",
    "    doc_name not ILIKE '%Presentation%' AND\n",
    "    doc_name not ILIKE '%Infographic%' AND\n",
    "    doc_name not ILIKE '%Guidence Note%' AND\n",
    "    doc_name not ILIKE '%Chapter%' AND\n",
    "    doc_name not ILIKE '%Module%' AND\n",
    "    lang_name = 'English' AND\n",
    "    cntry_name in ('Ghana','Ghana|N/A', 'Pakistan', 'Congo, Democratic Republic of','Kenya','Africa|Kenya', \n",
    "                    'Tunisia', 'Malawi','Uganda','Cambodia')\n",
    ")\n",
    "AND doc_date != 'Disclosed'\n",
    "AND disclsr_stat_name = 'Disclosed'\n",
    "ORDER BY doc_date DESC\n",
    "\"\"\"\n",
    "select_countries = []\n",
    "filtered_df = spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e26ea413-898f-4eb1-8b2e-e7aee7654ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filterd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba58ca7-a695-4d02-b78a-27477c416175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "failed_urls = []\n",
    "\n",
    "def fetch_text(url):\n",
    "    try:\n",
    "        if not url or not isinstance(url, str):\n",
    "            failed_urls.append((url, \"Invalid URL\"))\n",
    "            return None\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            failed_urls.append((url, f\"Status code: {response.status_code}\"))\n",
    "            return None\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        failed_urls.append((url, str(e)))\n",
    "        return None\n",
    "    \n",
    "filtered_df[\"doc_text\"] = filtered_df[\"ext_text_url\"].progress_apply(fetch_text)\n",
    "\n",
    "print(f\"Total failed fetches: {len(failed_urls)}\")\n",
    "for url, reason in failed_urls:\n",
    "    print(f\"Failed: {url} â€” {reason}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5458534-cd06-4df4-a262-ca4cbad91ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "database_name = \"prd_mega.sboost4\"\n",
    "\n",
    "if not spark.catalog.databaseExists(database_name):\n",
    "    print(f\"Database '{database_name}' does not exist. Creating the database.\")\n",
    "    spark.sql(f\"CREATE DATABASE {database_name}\")\n",
    "\n",
    "sdf = spark.createDataFrame(filtered_df)\n",
    "sdf = sdf.select([\n",
    "    F.col(c).cast(\"string\") if str(sdf.schema[c].dataType) == 'void' else F.col(c)\n",
    "    for c in sdf.columns\n",
    "])\n",
    "sdf.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.per_pfr_document_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fd55ef-eb31-49a6-aa92-927ab61ca6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# since the extracted text is in complicated formats, we use a simple chunker to start\n",
    "def chunk_text_by_chars(text, max_chars=5000):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), max_chars):\n",
    "        chunk = text[i:i + max_chars].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "chunk_data = []\n",
    "for row in filtered_df.iterrows():\n",
    "    node_id = str(row[1]['node_id'])\n",
    "    text = row[1]['doc_text']\n",
    "    data = [{'node_id':node_id, 'chunk_id':i, 'chunk_text':chunk} for i, chunk in enumerate(chunk_text_by_chars(text))]\n",
    "    chunk_data.extend(data)\n",
    "\n",
    "chunks_df = pd.DataFrame(chunk_data)\n",
    "\n",
    "database_name = \"prd_mega.sboost4\"\n",
    "\n",
    "if not spark.catalog.databaseExists(database_name):\n",
    "    print(f\"Database '{database_name}' does not exist. Creating the database.\")\n",
    "    spark.sql(f\"CREATE DATABASE {database_name}\")\n",
    "\n",
    "sdf_chunks = spark.createDataFrame(chunks_df)\n",
    "sdf_chunks.write.mode(\"overwrite\").saveAsTable(f\"{database_name}.per_pfr_chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d2e6cc0-14b8-4653-a79b-68054cacbabd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7120332998158039,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "data_extraction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
