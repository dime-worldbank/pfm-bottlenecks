{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb09b77-46c8-4a18-80c5-6de6016f52f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install instructor\n",
    "!pip install azure.identity openai\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77383731-38b0-4ca1-8769-54848bebd622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from time import sleep\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import instructor\n",
    "from pydantic import create_model, BaseModel\n",
    "from enum import Enum\n",
    "from typing import Optional, Type\n",
    "import requests\n",
    "from abc import ABC\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import get_bearer_token_provider, ClientSecretCredential\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7162fb7-7d7e-4ae6-b190-cd6ed6148755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "import json\n",
    "import unicodedata\n",
    "from pyspark.sql.functions import col, lower, regexp_extract, regexp_replace, when, lit, substring, expr, floor, concat, udf, lpad\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "from glob import glob\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a2191ea-09e0-4d46-b15f-56998a137220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# client set-up\n",
    "credential = ClientSecretCredential(\n",
    "       tenant_id = dbutils.secrets.get(scope=\"DAPGPTKEYVAULT\", key=\"GPT-APIM-Tenant-ID\"),\n",
    "       client_id = dbutils.secrets.get(scope=\"DAPGPTKEYVAULT\", key=\"GPT-APIM-Client-ID\"),\n",
    "       client_secret = dbutils.secrets.get(scope=\"DAPGPTKEYVAULT\", key=\"GPT-APIM-Client-Secret\")\n",
    "   )\n",
    "# set up Azure AD token provider\n",
    "token_provider = get_bearer_token_provider(credential, dbutils.secrets.get(scope=\"DAPGPTKEYVAULT\", key=\"GPT-APIM-Token-Cred\"))\n",
    "   \n",
    "#dbutils.secrets.get(scope=\"DAPGPTKEYVAULT\", key=\"GPT-APIM-Token-Cred\"\n",
    "# initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://azapim.worldbank.org/conversationalai/v2/\",\n",
    "    azure_ad_token_provider=token_provider,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "class BaseService(ABC):\n",
    "    \"\"\"\n",
    "    Base class for all services. Automatically registers subclasses with the ServiceRegistry.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init_subclass__(cls, **kwargs):\n",
    "        \"\"\"\n",
    "        Automatically registers subclasses with the ServiceRegistry when they are defined.\n",
    "        \"\"\"\n",
    "        super().__init_subclass__(**kwargs)\n",
    "\n",
    "        if not hasattr(cls, \"service_name\") or not isinstance(cls.service_name, str):\n",
    "            raise AttributeError(\n",
    "                f\"Service class '{cls.__name__}' must define a string 'service_name'.\"\n",
    "            )\n",
    "\n",
    "\n",
    "class AzureOpenAIService(BaseService):\n",
    "    \"\"\"\n",
    "    A service for interacting with Azure OpenAI using Instructor.\n",
    "    \"\"\"\n",
    "\n",
    "    service_name = \"azureopenai\"\n",
    "    \n",
    "    DEFAULT_SYSTEM_PROMPT = '''You are a public finance expert working with a multilateral development institution...\n",
    "\n",
    "    (same as before)\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client: AzureOpenAI,\n",
    "        system_prompt: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the AzureOpenAIService with an existing AzureOpenAI client.\n",
    "        \"\"\"\n",
    "        self.SYSTEM_PROMPT = system_prompt or self.DEFAULT_SYSTEM_PROMPT\n",
    "\n",
    "        # Wrap your AzureOpenAI client using instructor\n",
    "        self.client = instructor.from_openai(\n",
    "            client,\n",
    "            mode=instructor.Mode.JSON,\n",
    "        )\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        model: str,\n",
    "        response_model: Type,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        system_message: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Executes a structured chat completion.\n",
    "        \"\"\"\n",
    "        if not isinstance(prompt, str) or not prompt.strip():\n",
    "            raise ValueError(\"The 'prompt' must be a non-empty string.\")\n",
    "        if not isinstance(model, str):\n",
    "            raise ValueError(\"The 'model' must be a string.\")\n",
    "\n",
    "        system_message = system_message or self.SYSTEM_PROMPT\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                response_model=response_model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                **kwargs,\n",
    "            )\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error in AzureOpenAI API call: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee150e78-52aa-454a-b1c0-9610b88d55f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ConfidenceLevel(str, Enum):\n",
    "    strong = \"strong\"\n",
    "    borderline = \"borderline\"\n",
    "    weak = \"weak\"\n",
    "    \n",
    "class ValidationConfidence(str, Enum):\n",
    "    high = \"high\"\n",
    "    medium = \"medium\"\n",
    "    low = \"low\"\n",
    "\n",
    "class BottleneckBase(BaseModel):\n",
    "    confidence: Optional[ConfidenceLevel] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"How confidently the extracted evidence supports the bottleneck. \"\n",
    "            \"Choose 'strong' if the evidence clearly and directly supports the bottleneck, \"\n",
    "            \"'borderline' if it is somewhat relevant but may be open to interpretation, \"\n",
    "            \"and 'weak' if the evidence is tenuous, ambiguous, or only indirectly related.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "pf_challenges = [\n",
    "    {\n",
    "        \"challenge_id\": 1,\n",
    "        \"role_of_public_finance\": \"Commitment to Feasible Policy\",\n",
    "        \"role_description\": (\n",
    "            \"Finance ministries and other central agencies play a fundamental coordinating role when it comes to policy decisions, \"\n",
    "            \"functioning as a clearing house for various policy proposals across sector institutions, promoting dialogue and consultation around policy trade-offs, \"\n",
    "            \"and ultimately linking policy objectives with resource availability, mobilization and use. \"\n",
    "            \"This helps build commitment to policy decisions that are made and improves their feasibility and the likelihood that they will be implemented.\"\n",
    "        ),\n",
    "        \"challenge_name\": \"Insufficient Stakeholder Commitment to Policy Action\",\n",
    "        \"challenge_description\": (\n",
    "            \"Focuses on whether political and technical stakeholders demonstrate sustained commitment to implementing approved policies, \"\n",
    "            \"including challenges around ownership, continuity, and buy-in.\"\n",
    "        ),\n",
    "        \"bottlenecks\": [\n",
    "            {\n",
    "                \"bottleneck_id\": \"1.1\",\n",
    "                \"bottleneck_name\": \"Inadequate Commitment of Political and Technical Leadership\",\n",
    "                \"bottleneck_description\": (\n",
    "                    \"This bottleneck applies when there is a clear lack of sustained commitment by political or technical leaders to implement approved policies. \"\n",
    "                    \"This includes delays, resistance, or failure to act when reforms threaten the status quo, require politically difficult trade-offs, \"\n",
    "                    \"or demand resource shifts that are not followed through despite stated priorities. \"\n",
    "                    \"Examples include: approved reforms not being enacted, persistent underfunding of a priority despite commitments, or misalignment between stated goals and actual budget execution. \"\n",
    "                    \"Do **not** classify general governance weakness, vague statements, or budget/funding gaps **unless** directly tied to political/technical unwillingness or inaction. \"\n",
    "                    \"Be careful to distinguish from other bottlenecks like 2.1 (coordination failures), 5.2 (disconnect between budgets and policy), or 6.3 (weak execution).\"\n",
    "                ),\n",
    "                \"model_key\": \"bottleneck_1_1\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "class Bottleneck_1_1(BottleneckBase):\n",
    "    extracted_evidence: Optional[str] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Verbatim excerpt from the text that provides concrete evidence of political or technical leadership failing to commit to implementing approved policies. \"\n",
    "            \"Relevant evidence may include examples of delays, failure to act, resistance to disrupting the status quo, or refusal to reallocate resources despite stated goals. \"\n",
    "            \"Mere descriptions of policy ambitions or general intentions are not sufficient. \"\n",
    "            \"Focus only on failures of action, follow-through, or resource application. \"\n",
    "            \"Use only direct text from the source; do not paraphrase or infer.\"\n",
    "        )\n",
    "    )\n",
    "    reasoning: Optional[str] = Field(\n",
    "        None,\n",
    "        description=(\n",
    "            \"Short explanation of how the extracted text demonstrates weak or absent leadership commitment. \"\n",
    "            \"The reasoning must be grounded entirely in the quoted text and explain how inaction, delay, or resistance is evident.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "class BottleneckValidation_1_1(BaseModel):\n",
    "    # Reform or policy presence\n",
    "    mentions_approved_reform: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the text explicitly refers to a specific reform, policy, program, recommendation, or budget measure that has been officially approved, agreed upon, or endorsed by government or leadership. Do not mark true for general ambitions, intentions, or proposed reforms without clear approval.\"\n",
    "    )\n",
    "\n",
    "    reform_not_followed_through: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the text states or clearly describes that the approved reform or policy has not been implemented, enacted, or operationalized. Do not infer this based on slow progress or ambiguous phrasing.\"\n",
    "    )\n",
    "\n",
    "    followthrough_failure_attributed_to_leadership: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the failure to act is explicitly linked to political or technical leadership. The text must name or clearly point to decision-makers (e.g., Cabinet, ministry, Parliament) as responsible. Do not mark true if no actors are identified or if alternative causes are equally plausible.\"\n",
    "    )\n",
    "\n",
    "    # Resistance or interference\n",
    "    political_resistance_described: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text clearly describes political actors actively resisting, avoiding, or opposing a proposed or ongoing reform. Must include reference to politicians, political parties, or government decision-makers.\"\n",
    "    )\n",
    "\n",
    "    resistance_due_to_political_cost: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the resistance is clearly described as stemming from political cost, disruption of patronage, fear of losing influence, or other vested interests. Do not mark true if the cost is described in technical, not political, terms.\"\n",
    "    )\n",
    "\n",
    "    interference_in_execution: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text describes political or technical actors interfering with, overriding, or distorting the execution of an approved reform or budgeted action.\"\n",
    "    )\n",
    "\n",
    "    interference_is_discretionary: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the interference is described as intentional, discretionary, or politically motivated—not due to weak capacity, technical errors, or administrative bottlenecks.\"\n",
    "    )\n",
    "\n",
    "    # Resource prioritization and tradeoffs\n",
    "    failure_to_prioritize_resources: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text explicitly describes leadership or ministry unwillingness or failure to reallocate, constrain, or prioritize resources when tradeoffs are necessary. Do not mark true for passive budget misalignment or general underfunding unless refusal to act is clearly shown.\"\n",
    "    )\n",
    "\n",
    "    # Passive signals\n",
    "    demoralization_or_abandonment_described: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text describes clear symptoms such as demoralization, disengagement, abandonment of reform, or loss of momentum. These must reflect failure of implementation or leadership follow-through.\"\n",
    "    )\n",
    "\n",
    "    passive_signals_linked_to_leadership: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the above symptoms are explicitly or plausibly linked to disengagement or inaction by named political or technical leadership. Do not infer this if no actor is identified.\"\n",
    "    )\n",
    "\n",
    "    # Top-down failure\n",
    "    central_decision_harmed_implementation: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text shows that a central or national-level decision (or inaction) undermined subnational or delegated implementation or follow-through.\"\n",
    "    )\n",
    "\n",
    "    failure_due_to_top_level_coordination: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the issue is clearly attributed to a failure of leadership coordination, consultation, or direction—not to generic system design problems.\"\n",
    "    )\n",
    "\n",
    "    actor_named_or_identifiable: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if a responsible political or technical actor (e.g., a named ministry, Cabinet, Parliament) is explicitly mentioned or clearly implied in the text.\"\n",
    "    )\n",
    "\n",
    "    cause_of_inaction_explicit: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True only if the cause of inaction is clearly described as reluctance, disinterest, avoidance, resistance, or disengagement by leadership. Do not infer this from neutral language like 'progress is slow'.\"\n",
    "    )\n",
    "\n",
    "    reform_tied_to_government_commitment: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the reform or priority is described as originating from government commitments, statements, or strategies—not just from donor reports, technical plans, or analyst recommendations.\"\n",
    "    )\n",
    "\n",
    "    alternative_explanations_ruled_out: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text rules out other plausible causes for inaction—such as technical constraints, funding limitations, or institutional capacity gaps. Do not mark true if ambiguity remains.\"\n",
    "    )\n",
    "\n",
    "    uses_conditional_language: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text uses conditional or speculative phrases (e.g., 'should', 'could', 'must', 'may') instead of making clear statements about what has or has not happened.\"\n",
    "    )\n",
    "\n",
    "    too_vague_or_generic: Optional[bool] = Field(\n",
    "        None,\n",
    "        description=\"True if the text lacks specific examples, actor mentions, or implementation details and instead offers general complaints or aspirations.\"\n",
    "    )\n",
    "\n",
    "    fits_other_bottleneck: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"If the chunk better supports another bottleneck (e.g., 2.1, 5.2, 5.3, 8.2), indicate which. Leave blank if unsure.\"\n",
    "    )\n",
    "\n",
    "    # Final validation judgment\n",
    "    is_valid: bool = Field(\n",
    "        ...,\n",
    "        description=\"True only if the evidence clearly and directly supports Bottleneck 1.1 according to the criteria above. False if vague, misattributed, inferential, or better fits another bottleneck.\"\n",
    "    )\n",
    "\n",
    "    validation_reasoning: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Explain how the answers to the above flags led you to your decision. Highlight any red flags or failure to meet admission conditions.\"\n",
    "    )\n",
    "\n",
    "    confidence: Optional[ConfidenceLevel] = Field(\n",
    "        None,\n",
    "        description=\"How confidently you judge this to support Bottleneck 1.1: 'strong' for clear and direct evidence, 'borderline' for partial or ambiguous support, 'weak' for tenuous or indirect relevance.\"\n",
    "    )\n",
    "\n",
    "\n",
    "MODEL_REGISTRY = {\n",
    "    \"1.1\": Bottleneck_1_1,\n",
    "}\n",
    "        \n",
    "VALIDATION_MODEL_REGISTRY = {\n",
    "    \"1.1\": BottleneckValidation_1_1,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def make_bottleneck_prompt(\n",
    "    text: str,\n",
    "    role_of_public_finance: str,\n",
    "    role_description: str,\n",
    "    challenge_name: str,\n",
    "    challenge_description: str,\n",
    "    bottleneck_name: str,\n",
    "    bottleneck_description: str,\n",
    "    bottleneck_examples: Optional[list[str]] = None\n",
    ") -> str:\n",
    "\n",
    "    example_section = \"\"\n",
    "    if bottleneck_examples:\n",
    "        formatted = \"\\n\".join(f\"- {ex}\" for ex in bottleneck_examples)\n",
    "        example_section = f\"\"\"\n",
    "        Examples of valid evidence for this bottleneck include:\n",
    "        {formatted}\n",
    "        \"\"\".strip()\n",
    "\n",
    "    return f\"\"\"\n",
    "        You are analyzing a public finance document to identify specific bottlenecks affecting development outcomes.\n",
    "        \n",
    "        The context for your analysis is as follows:\n",
    "        \n",
    "        Role of Public Finance: {role_of_public_finance}\n",
    "        → {role_description}\n",
    "        \n",
    "        PFM Challenge: {challenge_name}\n",
    "        → {challenge_description}\n",
    "        \n",
    "        Specific Bottleneck: {bottleneck_name}\n",
    "        → {bottleneck_description}\n",
    "        \n",
    "        {example_section}\n",
    "        \n",
    "        ---\n",
    "        \n",
    "        Your task:\n",
    "        \n",
    "        - Carefully read the excerpt below.\n",
    "        - Extract direct evidence from the text that clearly supports the presence of the specific bottleneck listed above.\n",
    "        - Only extract text that is explicitly present in the excerpt.\n",
    "        - Do not infer, assume, or include information that is not stated.\n",
    "        - If you find no clear evidence, return null.\n",
    "        \n",
    "        For each piece of extracted evidence, briefly explain your reasoning (i.e., why this excerpt indicates the bottleneck), and indicate if the match is ambiguous.\n",
    "        \n",
    "        Text to analyze:\n",
    "        \n",
    "        {text}\n",
    "        \"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "def make_validation_prompt(\n",
    "    extracted_evidence: str,\n",
    "    reasoning: str,\n",
    "    role_of_public_finance: str,\n",
    "    role_description: str,\n",
    "    challenge_name: str,\n",
    "    challenge_description: str,\n",
    "    bottleneck_name: str,\n",
    "    bottleneck_description: str,\n",
    "    validation_model_cls=None,\n",
    "    bottleneck_examples: Optional[list[str]] = None\n",
    ") -> str:\n",
    "    # Get bottleneck-specific guidance if available\n",
    "    bottleneck_specific_criteria = \"\"\n",
    "    if validation_model_cls and hasattr(validation_model_cls, \"validation_guidance\"):\n",
    "        bottleneck_specific_criteria = validation_model_cls.validation_guidance()\n",
    "\n",
    "    # Format examples, if provided\n",
    "    example_text_block = \"\"\n",
    "    if bottleneck_examples:\n",
    "        formatted_examples = \"\\n\".join(f\"- {ex}\" for ex in bottleneck_examples)\n",
    "        example_text_block = (\n",
    "            \"Here are some representative examples of valid evidence for this bottleneck:\\n\"\n",
    "            f\"{formatted_examples}\"\n",
    "        )\n",
    "\n",
    "    # Compose the full prompt\n",
    "    return f\"\"\"\n",
    "        You are validating whether a previously extracted piece of text is strong evidence of a specific bottleneck in public finance.\n",
    "\n",
    "        Here is the context:\n",
    "\n",
    "        Role of Public Finance: {role_of_public_finance}\n",
    "        → {role_description}\n",
    "\n",
    "        PFM Challenge: {challenge_name}\n",
    "        → {challenge_description}\n",
    "\n",
    "        Specific Bottleneck: {bottleneck_name}\n",
    "        → {bottleneck_description}\n",
    "\n",
    "        {(\"Additional evaluation criteria:\" + bottleneck_specific_criteria) if bottleneck_specific_criteria else \"\"}\n",
    "        {f\"{example_text_block}\" if example_text_block else \"\"}\n",
    "\n",
    "        ---\n",
    "\n",
    "        Extracted Evidence:\n",
    "        {extracted_evidence}\n",
    "\n",
    "        Reasoning for Extraction:\n",
    "        {reasoning}\n",
    "\n",
    "        ---\n",
    "\n",
    "        Your task:\n",
    "\n",
    "        - First, evaluate whether the evidence is general, misplaced, insufficient, or misclassified using the bottleneck-specific criteria above.\n",
    "        - Then, reflect on those evaluations to decide:\n",
    "            - Does this evidence clearly and directly support the bottleneck?\n",
    "            - Is the reasoning plausible and grounded in the evidence?\n",
    "        - If the evidence is vague, general, or fits another bottleneck better, mark `is_valid` as False.\n",
    "        - Ensure the `is_valid` field reflects your judgment based on the flags you select.\n",
    "        - Your validation reasoning must explain how the intermediate evaluations informed your final decision.\n",
    "        \"\"\".strip()\n",
    "\n",
    "        \n",
    "def make_structured_summary_prompt(\n",
    "    context_text: str,\n",
    "    extracted_evidence: str,\n",
    "    bottleneck_name: str,\n",
    "    bottleneck_description: str,\n",
    "    metadata_tuple: tuple\n",
    ") -> str:\n",
    "    country, doc_name, region, topic_text = metadata_tuple\n",
    "    return f\"\"\"\n",
    "You are summarizing validated evidence from a public finance diagnostic document.\n",
    "\n",
    "Document Metadata:\n",
    "- Country: {country}\n",
    "- Document: {doc_name}\n",
    "- Region: {region}\n",
    "- Issue Area: {topic_text}\n",
    "\n",
    "Bottleneck Focus:\n",
    "- {bottleneck_name}\n",
    "→ {bottleneck_description}\n",
    "\n",
    "Text Context:\n",
    "\\\"\\\"\\\"{context_text}\\\"\\\"\\\"\n",
    "\n",
    "Extracted Evidence:\n",
    "\\\"\\\"\\\"{extracted_evidence}\\\"\\\"\\\"\n",
    "\n",
    "Your task:\n",
    "Write a concise 1–2 sentence summary that:\n",
    "- Names the **specific challenge**\n",
    "- Describes its **consequence**\n",
    "- Mentions the relevant **policy or issue area** if identifiable\n",
    "\n",
    "Avoid jargon, hedging, or hallucination.\n",
    "\"\"\".strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4bfa76-e543-4f01-8083-8530d4815583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ACTIVE_BOTTLENECK_IDS = [\"1.1\"]\n",
    "active_bottlenecks = []\n",
    "for challenge in pf_challenges:\n",
    "    for b in challenge[\"bottlenecks\"]:\n",
    "        if b[\"bottleneck_id\"] in ACTIVE_BOTTLENECK_IDS:\n",
    "            active_bottlenecks.append({\n",
    "                \"challenge_name\": challenge[\"challenge_name\"],\n",
    "                \"challenge_id\": challenge[\"challenge_id\"],\n",
    "                \"challenge_description\": challenge[\"challenge_description\"],\n",
    "                \"role_of_public_finance\": challenge[\"role_of_public_finance\"],\n",
    "                \"role_description\": challenge[\"role_description\"],\n",
    "                **b\n",
    "            })\n",
    "sys_prompt = \"\"\"\n",
    "You are a public finance expert working at a multilateral development bank. \n",
    "Your job is to identify and evaluate evidence of bottlenecks in Public Financial Management (PFM) systems, \n",
    "based on diagnostic reports, budget support documents, and public expenditure reviews.\n",
    "\n",
    "You are especially trained to distinguish between:\n",
    "- vague or aspirational language vs. concrete implementation gaps,\n",
    "- general governance issues vs. specific failures of leadership commitment,\n",
    "- PFM system reform vs. follow-through on approved policy actions.\n",
    "\n",
    "Only validate a chunk as evidence for a bottleneck if it meets the precise criteria described.\n",
    "\"\"\"\n",
    "service = AzureOpenAIService(client=client, system_prompt=sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0216457-9d82-4a7d-8811-32b6e16daea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=\"source_chunks\")\n",
    "def load_chunks():\n",
    "    node_ids = [\n",
    "        \"9612673\", \"8846482\", \"5669851\"\n",
    "    ]\n",
    "\n",
    "    return (\n",
    "        spark.read.table(\"prd_mega.sboost4.per_pfr_chunks\")\n",
    "        .filter(col(\"node_id\").isin(node_ids))\n",
    "        .select(\"node_id\", \"chunk_id\", \"text\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e26e4f83-3100-4b07-8e26-c52d6d65ddf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=\"bottleneck_extractions\")\n",
    "def extract_evidence():\n",
    "    df = dlt.read(\"source_chunks\").toPandas()\n",
    "\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        for b in active_bottlenecks:\n",
    "            model_cls = MODEL_REGISTRY[b[\"bottleneck_id\"]]\n",
    "            prompt = make_bottleneck_prompt(\n",
    "                text=row.text,\n",
    "                role_of_public_finance=b[\"role_of_public_finance\"],\n",
    "                role_description=b[\"role_description\"],\n",
    "                challenge_name=b[\"challenge_name\"],\n",
    "                challenge_description=b[\"challenge_description\"],\n",
    "                bottleneck_name=b[\"bottleneck_name\"],\n",
    "                bottleneck_description=b[\"bottleneck_description\"],\n",
    "                bottleneck_examples=b.get(\"boAttleneck_examples\"),\n",
    "            )\n",
    "            try:\n",
    "                result = service.execute(prompt, model=\"gpt-4o\", response_model=model_cls)\n",
    "                results.append({\n",
    "                    \"node_id\": row.node_id,\n",
    "                    \"chunk_id\": row.chunk_id,\n",
    "                    \"bottleneck_id\": b[\"bottleneck_id\"],\n",
    "                    \"extracted_evidence\": result.extracted_evidence,\n",
    "                    \"reasoning\": result.reasoning,\n",
    "                    \"extraction_confidence\": str(result.confidence) \n",
    "                })\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"node_id\", StringType()),\n",
    "        StructField(\"chunk_id\", StringType()),\n",
    "        StructField(\"bottleneck_id\", StringType()),\n",
    "        StructField(\"extracted_evidence\", StringType()),\n",
    "        StructField(\"reasoning\", StringType()),\n",
    "        StructField(\"extraction_confidence\", StringType()),  \n",
    "    ])\n",
    "\n",
    "    return spark.createDataFrame(results or [], schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f87c807d-d28c-4978-9fc1-d2082fdd8f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=\"bottleneck_validations\")\n",
    "def validate_evidence():\n",
    "    df = dlt.read(\"bottleneck_extractions\").toPandas()\n",
    "\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        config = next(b for b in active_bottlenecks if b[\"bottleneck_id\"] == row.bottleneck_id)\n",
    "        validation_cls = VALIDATION_MODEL_REGISTRY.get(row.bottleneck_id)\n",
    "\n",
    "        if not validation_cls:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            prompt = make_validation_prompt(\n",
    "                extracted_evidence=row.extracted_evidence,\n",
    "                reasoning=row.reasoning,\n",
    "                role_of_public_finance=config[\"role_of_public_finance\"],\n",
    "                role_description=config[\"role_description\"],\n",
    "                challenge_name=config[\"challenge_name\"],\n",
    "                challenge_description=config[\"challenge_description\"],\n",
    "                bottleneck_name=config[\"bottleneck_name\"],\n",
    "                bottleneck_description=config[\"bottleneck_description\"],\n",
    "                validation_model_cls=validation_cls\n",
    "            )\n",
    "\n",
    "            val = service.execute(\n",
    "                prompt,\n",
    "                model=\"gpt-4o\",\n",
    "                response_model=validation_cls\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"node_id\": row.node_id,\n",
    "                \"chunk_id\": row.chunk_id,\n",
    "                \"bottleneck_id\": row.bottleneck_id,\n",
    "                \"is_valid\": val.is_valid,\n",
    "                \"validation_reasoning\": val.validation_reasoning,\n",
    "                \"validation_confidence\": val.confidence,\n",
    "                **val.model_dump(exclude_unset=True, exclude={\"validation_reasoning\", \"validation_confidence\", \"is_valid\"})\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Validation error: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Handle empty results case\n",
    "    if not results:\n",
    "        from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "        schema = StructType([\n",
    "            StructField(\"node_id\", StringType()),\n",
    "            StructField(\"chunk_id\", StringType()),\n",
    "            StructField(\"bottleneck_id\", StringType()),\n",
    "            StructField(\"is_valid\", BooleanType()),\n",
    "            StructField(\"validation_reasoning\", StringType()),\n",
    "            StructField(\"validation_confidence\", StringType())\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema)\n",
    "\n",
    "    return spark.createDataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48cda1f5-65cc-4238-a8f7-401e44186b4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(name=\"bottleneck_summaries\")\n",
    "def summarize_validated_chunks():\n",
    "    import pandas as pd\n",
    "\n",
    "    df = dlt.read(\"bottleneck_validations\").filter(\"is_valid = true\").toPandas()\n",
    "\n",
    "    results = []\n",
    "    for row in df.itertuples():\n",
    "        try:\n",
    "            prompt = make_structured_summary_prompt(\n",
    "                context_text=row.chunk,\n",
    "                extracted_evidence=row.extracted_evidence,\n",
    "                bottleneck_name=row.bottleneck,\n",
    "                bottleneck_description=row.bottleneck_description,\n",
    "                metadata_tuple=(\n",
    "                    row.cntry_name,\n",
    "                    row.doc_name,\n",
    "                    getattr(row, \"region\", \"\"),  # safer than hasattr for pandas rows\n",
    "                    row.ent_topic_text,\n",
    "                )\n",
    "            )\n",
    "            summary = service.execute(prompt, model=\"gpt-4o\", response_model=str)\n",
    "            results.append({\n",
    "                \"node_id\": row.node_id,\n",
    "                \"chunk_id\": row.chunk_id,\n",
    "                \"bottleneck_id\": row.bottleneck_id,\n",
    "                \"summary\": summary.strip()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing {row.node_id}/{row.chunk_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Fallback if empty\n",
    "    if not results:\n",
    "        from pyspark.sql.types import StructType, StructField, StringType\n",
    "        schema = StructType([\n",
    "            StructField(\"node_id\", StringType()),\n",
    "            StructField(\"chunk_id\", StringType()),\n",
    "            StructField(\"bottleneck_id\", StringType()),\n",
    "            StructField(\"summary\", StringType()),\n",
    "        ])\n",
    "        return spark.createDataFrame([], schema)\n",
    "\n",
    "    return spark.createDataFrame(results)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bottleneck_transform_load_dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
